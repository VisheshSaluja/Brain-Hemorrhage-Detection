{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import get_CIFAR10_data from data_process.py\n",
    "from data_process import get_CIFAR10_data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load data\n",
    "data = get_CIFAR10_data()\n",
    "\n",
    "# Retrieve datasets\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val, y_val = data[\"X_val\"], data[\"y_val\"]\n",
    "X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "\n",
    "# Convert to PyTorch tensors and apply transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_train = torch.tensor(X_train).float() / 255  # Normalizing to [0, 1] range\n",
    "X_val = torch.tensor(X_val).float() / 255\n",
    "X_test = torch.tensor(X_test).float() / 255\n",
    "y_train = torch.tensor(y_train)\n",
    "y_val = torch.tensor(y_val)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        layers = [block(self.in_planes, planes, stride)]\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = nn.AvgPool2d(4)(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate ResNet for CIFAR-10 (ResNet-18)\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishesh/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.8730357297095555\n",
      "Epoch [2/100], Loss: 1.3218409003539122\n",
      "Epoch [3/100], Loss: 1.0304363471098419\n",
      "Epoch [4/100], Loss: 0.8139414171014692\n",
      "Epoch [5/100], Loss: 0.6444653789306123\n",
      "Epoch [6/100], Loss: 0.5206202747927633\n",
      "Epoch [7/100], Loss: 0.44646078908412323\n",
      "Epoch [8/100], Loss: 0.3922140416604109\n",
      "Epoch [9/100], Loss: 0.3480944662274642\n",
      "Epoch [10/100], Loss: 0.31240905478946845\n",
      "Accuracy on the test set after epoch 10: 77.83%\n",
      "Epoch [11/100], Loss: 0.27871740593844857\n",
      "Epoch [12/100], Loss: 0.26819978049389687\n",
      "Epoch [13/100], Loss: 0.25038716803954103\n",
      "Epoch [14/100], Loss: 0.2316042138920129\n",
      "Epoch [15/100], Loss: 0.22349707383477657\n",
      "Epoch [16/100], Loss: 0.21917569493344807\n",
      "Epoch [17/100], Loss: 0.21041642072034256\n",
      "Epoch [18/100], Loss: 0.20324311629132252\n",
      "Epoch [19/100], Loss: 0.195562837076405\n",
      "Epoch [20/100], Loss: 0.1998163971942959\n",
      "Accuracy on the test set after epoch 20: 78.55%\n",
      "Epoch [21/100], Loss: 0.18591015428966082\n",
      "Epoch [22/100], Loss: 0.19310611082780765\n",
      "Epoch [23/100], Loss: 0.18581456079223138\n",
      "Epoch [24/100], Loss: 0.1940023712522056\n",
      "Epoch [25/100], Loss: 0.17539571646805222\n",
      "Epoch [26/100], Loss: 0.1825921872032529\n",
      "Epoch [27/100], Loss: 0.17947566389609565\n",
      "Epoch [28/100], Loss: 0.18050926960712935\n",
      "Epoch [29/100], Loss: 0.17410714785245007\n",
      "Epoch [30/100], Loss: 0.18127293964512353\n",
      "Accuracy on the test set after epoch 30: 80.01%\n",
      "Epoch [31/100], Loss: 0.1787693429222773\n",
      "Epoch [32/100], Loss: 0.1700053291672515\n",
      "Epoch [33/100], Loss: 0.1722050697564612\n",
      "Epoch [34/100], Loss: 0.17733741171813666\n",
      "Epoch [35/100], Loss: 0.17271926198592386\n",
      "Epoch [36/100], Loss: 0.17001919551477732\n",
      "Epoch [37/100], Loss: 0.1722555613879591\n",
      "Epoch [38/100], Loss: 0.1675945311473493\n",
      "Epoch [39/100], Loss: 0.1668278562201688\n",
      "Epoch [40/100], Loss: 0.1731408047866728\n",
      "Accuracy on the test set after epoch 40: 78.92%\n",
      "Epoch [41/100], Loss: 0.16240429593245292\n",
      "Epoch [42/100], Loss: 0.16934565505299803\n",
      "Epoch [43/100], Loss: 0.16198441680939016\n",
      "Epoch [44/100], Loss: 0.1642202878511917\n",
      "Epoch [45/100], Loss: 0.16840531597249503\n",
      "Epoch [46/100], Loss: 0.16898041936342773\n",
      "Epoch [47/100], Loss: 0.15850371157376014\n",
      "Epoch [48/100], Loss: 0.15908607598345523\n",
      "Epoch [49/100], Loss: 0.17036026662735965\n",
      "Epoch [50/100], Loss: 0.160315066527371\n",
      "Accuracy on the test set after epoch 50: 80.49%\n",
      "Epoch [51/100], Loss: 0.1636683647046668\n",
      "Epoch [52/100], Loss: 0.15839962730325233\n",
      "Epoch [53/100], Loss: 0.1638923432609898\n",
      "Epoch [54/100], Loss: 0.15989216539629444\n",
      "Epoch [55/100], Loss: 0.17038035811005312\n",
      "Epoch [56/100], Loss: 0.15956163216703242\n",
      "Epoch [57/100], Loss: 0.15932396038705313\n",
      "Epoch [58/100], Loss: 0.1571293248850901\n",
      "Epoch [59/100], Loss: 0.16023609971544917\n",
      "Epoch [60/100], Loss: 0.15477756129738246\n",
      "Accuracy on the test set after epoch 60: 77.81%\n",
      "Epoch [61/100], Loss: 0.1623072746372721\n",
      "Epoch [62/100], Loss: 0.15664230501060386\n",
      "Epoch [63/100], Loss: 0.16044587047658762\n",
      "Epoch [64/100], Loss: 0.15918076617093685\n",
      "Epoch [65/100], Loss: 0.1540483657718094\n",
      "Epoch [66/100], Loss: 0.15875246013969416\n",
      "Epoch [67/100], Loss: 0.1642222105581362\n",
      "Epoch [68/100], Loss: 0.15007635875174644\n",
      "Epoch [69/100], Loss: 0.16316375472916014\n",
      "Epoch [70/100], Loss: 0.14839279797540944\n",
      "Accuracy on the test set after epoch 70: 76.22%\n",
      "Epoch [71/100], Loss: 0.16515445136331391\n",
      "Epoch [72/100], Loss: 0.1589891082235167\n",
      "Epoch [73/100], Loss: 0.1527806724331709\n",
      "Epoch [74/100], Loss: 0.1609361456420185\n",
      "Epoch [75/100], Loss: 0.15347201559040938\n",
      "Epoch [76/100], Loss: 0.1625384586345278\n",
      "Epoch [77/100], Loss: 0.15578496326782373\n",
      "Epoch [78/100], Loss: 0.1482836430790219\n",
      "Epoch [79/100], Loss: 0.1549655928838191\n",
      "Epoch [80/100], Loss: 0.1537548488604182\n",
      "Accuracy on the test set after epoch 80: 81.42%\n",
      "Epoch [81/100], Loss: 0.15948523361206676\n",
      "Epoch [82/100], Loss: 0.14323980948710255\n",
      "Epoch [83/100], Loss: 0.153685331208432\n",
      "Epoch [84/100], Loss: 0.15490745108175838\n",
      "Epoch [85/100], Loss: 0.15526098311531014\n",
      "Epoch [86/100], Loss: 0.15835155981444504\n",
      "Epoch [87/100], Loss: 0.14921099781581534\n",
      "Epoch [88/100], Loss: 0.15223508248441214\n",
      "Epoch [89/100], Loss: 0.16690004392291174\n",
      "Epoch [90/100], Loss: 0.13722148451069005\n",
      "Accuracy on the test set after epoch 90: 79.44%\n",
      "Epoch [91/100], Loss: 0.16320169545885166\n",
      "Epoch [92/100], Loss: 0.150177366963488\n",
      "Epoch [93/100], Loss: 0.15619278314958665\n",
      "Epoch [94/100], Loss: 0.15809494660627438\n",
      "Epoch [95/100], Loss: 0.14672949215684172\n",
      "Epoch [96/100], Loss: 0.15138180595251194\n",
      "Epoch [97/100], Loss: 0.15410893341385354\n",
      "Epoch [98/100], Loss: 0.1522499359755074\n",
      "Epoch [99/100], Loss: 0.1535994814023103\n",
      "Epoch [100/100], Loss: 0.15504474046958644\n",
      "Accuracy on the test set after epoch 100: 80.29%\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device before initializing the optimizer\n",
    "net = net.to(device)\n",
    "\n",
    "# Define optimizer after moving model to the device\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to the correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        print(f'Accuracy on the test set after epoch {epoch+1}: {100 * correct / total:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot Training Loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[43mtrain_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Test Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
